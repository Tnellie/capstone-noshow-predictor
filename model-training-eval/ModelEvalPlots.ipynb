{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020fc187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from foundry.transforms import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33d5a49",
   "metadata": {},
   "source": [
    "### Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc475fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "def plot_precision_recall_curve(pr_curve_df, figsize=(10, 6)):\n",
    "    \"\"\"\n",
    "    Plot precision-recall curve from a PySpark DF\n",
    "    \n",
    "    Parameters:\n",
    "    pr_curve_df - DF with precision, recall and no_skill_precision columns\n",
    "    figsize - Figure size tuple (default: (10, 6))\n",
    "    \"\"\"\n",
    "    # Convert DF to Pandas for plotting\n",
    "    # pr_data = pr_curve_df.toPandas()\n",
    "    \n",
    "    # Get no-skill baseline value\n",
    "    no_skill = pr_curve_df['no_skill_precision'].iloc[0]\n",
    "    \n",
    "    # Plot precision-recall curve\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(pr_curve_df['recall'], pr_curve_df['precision'], \n",
    "             marker='.', linestyle='-', color='blue', label='Model')\n",
    "    \n",
    "    # Plot no-skill baseline\n",
    "    plt.plot([0, 1], [no_skill, no_skill], \n",
    "             linestyle='--', color='red', label='No Skill')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Recall', fontsize=12)\n",
    "    plt.ylabel('Precision', fontsize=12)\n",
    "    plt.title('Precision-Recall Curve', fontsize=14)\n",
    "    \n",
    "    # Add grid and legend\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(fontsize=10)\n",
    "    \n",
    "    # Set axis limits\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    \n",
    "    # Add a text with the avg precision score\n",
    "    # Calculate the avg area under PR curve\n",
    "    sorted_data = pr_curve_df.sort_values('recall')\n",
    "    ap = np.trapezoid(sorted_data['precision'], sorted_data['recall'])\n",
    "    plt.text(0.05, 0.05, f'Average Precision: {ap:.4f}', \n",
    "             transform=plt.gca().transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf6680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "def plot_confusion_matrix(confusion_df, normalize=False, title='Confusion Matrix', cmap='Blues', figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix from a PySpark DF containing confusion matrix data.\n",
    "    \n",
    "    Parameters:\n",
    "    confusion_df - DF with columns: actual, predicted, count\n",
    "    normalize - Whether to normalize by row (default: False)\n",
    "    title - Plot title (default: 'Confusion Matrix')\n",
    "    cmap - Colormap to use (default: 'Blues')\n",
    "    figsize - Figure size (default: (10, 8))\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib fig\n",
    "    \"\"\"\n",
    "    # Pivot data to create matrix format\n",
    "    matrix = confusion_df.pivot(index='actual', columns='predicted', values='count')\n",
    "    \n",
    "    # Create a 2x2 confusion matrix array - fill missing vals with 0\n",
    "    classes = sorted(set(confusion_df['actual'].unique()).union(set(confusion_df['predicted'].unique())))\n",
    "    cm_array = np.zeros((len(classes), len(classes)))\n",
    "    \n",
    "    for i, actual in enumerate(classes):\n",
    "        for j, predicted in enumerate(classes):\n",
    "            matching = confusion_df[(confusion_df['actual'] == actual) & \n",
    "                                (confusion_df['predicted'] == predicted)]\n",
    "            if not matching.empty:\n",
    "                cm_array[i, j] = matching['count'].iloc[0]\n",
    "    \n",
    "    # Normalize if requested\n",
    "    if normalize:\n",
    "        row_sums = cm_array.sum(axis=1)\n",
    "        cm_array = cm_array / row_sums[:, np.newaxis]\n",
    "        fmt = '.2f'\n",
    "    else:\n",
    "        # Convert to ints\n",
    "        if np.all(np.mod(cm_array, 1) == 0):\n",
    "            cm_array = cm_array.astype(int)\n",
    "            fmt = 'd'\n",
    "        else:\n",
    "            fmt = '.1f'\n",
    "    \n",
    "    # Create fig\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(cm_array, annot=True, fmt=fmt, cmap=cmap, cbar=False,\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    \n",
    "    # Set labels\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Add metrics to the plot\n",
    "    if cm_array.shape == (2, 2):\n",
    "        tn, fp = cm_array[0, 0], cm_array[0, 1]\n",
    "        fn, tp = cm_array[1, 0], cm_array[1, 1]\n",
    "        \n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        metrics_text = (f\"Accuracy: {accuracy:.3f}\\n\"\n",
    "                       f\"Precision: {precision:.3f}\\n\"\n",
    "                       f\"Recall: {recall:.3f}\\n\"\n",
    "                       f\"F1 Score: {f1:.3f}\")\n",
    "        \n",
    "        plt.figtext(1.05, 0.5, metrics_text, ha='left', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f24d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "def plot_roc_curve(roc_df, title='ROC Curve', figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Plot ROC curve from a PySpark DF containing ROC data.\n",
    "    \n",
    "    Parameters:\n",
    "    roc_df - PySpark DF with columns: fpr, tpr, threshold, auc\n",
    "    title - Plot title (default: 'ROC Curve')\n",
    "    figsize - Figure size (default: (10, 8))\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib fig\n",
    "    \"\"\"\n",
    "    # Get AUC value\n",
    "    auc = roc_df['auc'].iloc[0] if 'auc' in roc_df.columns else None\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Plot the ROC curve\n",
    "    plt.plot(roc_df['fpr'], roc_df['tpr'], \n",
    "             lw=2, color='blue', \n",
    "             label=f'ROC Curve (AUC = {auc:.3f})' if auc else 'ROC Curve')\n",
    "    \n",
    "    # Plot the random guess line\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', \n",
    "             label='Random Guess (AUC = 0.5)')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Add grid and legend\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    # Set axis limits\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    \n",
    "    # Add annotation for perfect point\n",
    "    plt.annotate('Perfect Classification', xy=(0, 1), xytext=(0.2, 0.8),\n",
    "                 arrowprops=dict(facecolor='green', shrink=0.05))\n",
    "    \n",
    "    # Add threshold markers (optional)\n",
    "    if 'threshold' in roc_df.columns:\n",
    "        # Select a few threshold points to display\n",
    "        thresholds_to_show = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "        \n",
    "        # Remove NA values before finding closest points\n",
    "        valid_threshold_df = roc_df.dropna(subset=['threshold'])\n",
    "        \n",
    "        # Only proceed if we have valid thresholds\n",
    "        if not valid_threshold_df.empty:\n",
    "            for threshold in thresholds_to_show:\n",
    "                # Find the closest point without triggering warnings\n",
    "                # Calculate absolute differences with the target threshold\n",
    "                abs_diffs = (valid_threshold_df['threshold'] - threshold).abs()\n",
    "                closest_idx = abs_diffs.idxmin()  # Get the index of the minimum difference\n",
    "                \n",
    "                # Get the corresponding point\n",
    "                point = valid_threshold_df.loc[closest_idx]\n",
    "                \n",
    "                # Plot the point and add annotation\n",
    "                plt.plot(point['fpr'], point['tpr'], 'ro', markersize=6)\n",
    "                plt.annotate(f\"t={threshold:.1f}\", \n",
    "                            (point['fpr'], point['tpr']),\n",
    "                            textcoords=\"offset points\",\n",
    "                            xytext=(0,10),\n",
    "                            ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed8d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lift chart\n",
    "def plot_lift_chart(lift_df, title=\"Lift Chart and Cumulative Gain\", figsize=(14, 8), save_path=None):\n",
    "    \"\"\"\n",
    "    Create lift chart and cumulative gains chart from lift data DF\n",
    "    \n",
    "    Parameters:\n",
    "    lift_df - DF with lift chart data\n",
    "    title - Plot title (default: \"Lift Chart and Cumulative Gain\")\n",
    "    figsize - Figure size (default: (14, 8))\n",
    "    save_path - Path to save the figure (default: None)\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib figure\n",
    "    \"\"\"\n",
    "    # Convert to pandas if needed\n",
    "    if not isinstance(lift_df, pd.DataFrame):\n",
    "        lift_pandas = lift_df.toPandas()\n",
    "    else:\n",
    "        lift_pandas = lift_df\n",
    "    \n",
    "    # Create fig with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Cumulative Gain Chart\n",
    "    ax1.plot(lift_pandas['percentile'], lift_pandas['model_cumulative_gain'], \n",
    "             'b-', linewidth=2, label='Model')\n",
    "    ax1.plot(lift_pandas['percentile'], lift_pandas['random_gain'], \n",
    "             'r--', linewidth=2, label='Random')\n",
    "    \n",
    "    if 'perfect_gain' in lift_pandas.columns:\n",
    "        ax1.plot(lift_pandas['percentile'], lift_pandas['perfect_gain'], \n",
    "                 'g-.', linewidth=2, label='Perfect')\n",
    "    \n",
    "    # Format axes\n",
    "    ax1.set_xlabel('Percentage of Sample', fontsize=12)\n",
    "    ax1.set_ylabel('Percentage of Positives Captured', fontsize=12)\n",
    "    ax1.set_title('Cumulative Gains Chart', fontsize=14)\n",
    "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax1.legend(fontsize=10)\n",
    "    \n",
    "    # Format tick labels as %s\n",
    "    ax1.xaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "    ax1.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "    \n",
    "    # Add reference lines\n",
    "    ax1.axhline(y=50, color='gray', linestyle='-', alpha=0.3)\n",
    "    ax1.axhline(y=80, color='gray', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Find and annotate locations of specific percentages of positives\n",
    "    for target_gain in [50, 80]:\n",
    "        # Find closest point to target gain\n",
    "        if not lift_pandas.empty and 'model_cumulative_gain' in lift_pandas:\n",
    "            closest_idx = (lift_pandas['model_cumulative_gain'] - target_gain).abs().idxmin()\n",
    "            x_pct = lift_pandas.loc[closest_idx, 'percentile']\n",
    "            \n",
    "            # Add vertical line and annotation\n",
    "            ax1.axvline(x=x_pct, color='blue', linestyle=':', alpha=0.5)\n",
    "            ax1.annotate(f'{target_gain}% of positives \\ncaptured at {x_pct:.1f}%', \n",
    "                         xy=(x_pct, target_gain), xytext=(x_pct+5, target_gain-10),\n",
    "                         arrowprops=dict(arrowstyle='->', color='blue', alpha=0.7))\n",
    "    \n",
    "    # Lift Chart\n",
    "    ax2.plot(lift_pandas['percentile'], lift_pandas['lift'], 'b-', linewidth=2)\n",
    "    ax2.axhline(y=1, color='r', linestyle='--', label='No Lift (Random)')\n",
    "    \n",
    "    # Format axes\n",
    "    ax2.set_xlabel('Percentage of Sample', fontsize=12)\n",
    "    ax2.set_ylabel('Lift', fontsize=12)\n",
    "    ax2.set_title('Lift Chart', fontsize=14)\n",
    "    ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax2.legend(fontsize=10)\n",
    "    \n",
    "    # Format tick labels\n",
    "    ax2.xaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "    \n",
    "    # Highlight the areas of highest lift\n",
    "    if not lift_pandas.empty and 'lift' in lift_pandas:\n",
    "        # Find point with max lift\n",
    "        max_lift_idx = lift_pandas['lift'].idxmax()\n",
    "        max_lift_percentile = lift_pandas.loc[max_lift_idx, 'percentile']\n",
    "        max_lift_value = lift_pandas.loc[max_lift_idx, 'lift']\n",
    "        \n",
    "        # Annotate max lift\n",
    "        ax2.annotate(f'Max Lift: {max_lift_value:.2f}x at {max_lift_percentile}%', \n",
    "                     xy=(max_lift_percentile, max_lift_value), \n",
    "                     xytext=(max_lift_percentile+10, max_lift_value),\n",
    "                     arrowprops=dict(arrowstyle='->', color='blue', alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.suptitle(title, y=1.05, fontsize=16)\n",
    "    \n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7f491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_importance(df, figsize=(12, 6)):\n",
    "    \"\"\"\n",
    "    Show cumulative importance to see how many features \n",
    "    account for 80%, 90%, 95% of total importance.\n",
    "    \"\"\"\n",
    "    df_sorted = df.copy()\n",
    "    df_sorted['cumulative_importance'] = df_sorted['importance'].cumsum()\n",
    "    df_sorted['cumulative_percentage'] = (df_sorted['cumulative_importance'] / \n",
    "                                          df_sorted['importance'].sum()) * 100\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Bar chart\n",
    "    top_20 = df_sorted.head(20)\n",
    "    ax1.barh(range(len(top_20)), top_20['importance'], color='steelblue')\n",
    "    ax1.set_yticks(range(len(top_20)))\n",
    "    ax1.set_yticklabels(top_20['feature'])\n",
    "    ax1.set_xlabel('Importance')\n",
    "    ax1.set_title('Top 20 Features')\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    # Cumulative line plot\n",
    "    ax2.plot(range(len(df_sorted)), df_sorted['cumulative_percentage'], \n",
    "             color='darkred', linewidth=2)\n",
    "    ax2.axhline(y=80, color='green', linestyle='--', label='80%')\n",
    "    ax2.axhline(y=90, color='orange', linestyle='--', label='90%')\n",
    "    ax2.axhline(y=95, color='red', linestyle='--', label='95%')\n",
    "    ax2.set_xlabel('Number of Features')\n",
    "    ax2.set_ylabel('Cumulative Importance (%)')\n",
    "    ax2.set_title('Cumulative Feature Importance')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print num features needed for thresholds\n",
    "    for threshold in [80, 90, 95]:\n",
    "        n_features = (df_sorted['cumulative_percentage'] <= threshold).sum() + 1\n",
    "        print(f\"{n_features} features account for {threshold}% of importance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242aa0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance_horizontal(df, top_n=20, figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Create a horizontal bar chart of top N feature importances.\n",
    "    \"\"\"\n",
    "    top_features = df.head(top_n)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'], color='steelblue')\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Importance', fontsize=12)\n",
    "    plt.ylabel('Feature', fontsize=12)\n",
    "    plt.title(f'Top {top_n} Feature Importances', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()  # Highest importance at top\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475bf4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_calibration_curve(df, n_bins=10, figsize=(10, 6)):\n",
    "    \"\"\"\n",
    "    Check if predicted probabilities match actual frequencies.\n",
    "    Perfect calibration = diagonal line.\n",
    "    \"\"\"\n",
    "    df['is_error'] = (df['noShow_day1_target'] != df['prediction']).astype(int)\n",
    "    \n",
    "    # Create probability bins\n",
    "    df['prob_bin'] = pd.cut(df['probability_positive'], bins=n_bins)\n",
    "    \n",
    "    # Calculate actual rate in each bin\n",
    "    calibration = df.groupby('prob_bin', observed=True).agg({\n",
    "        'noShow_day1_target': 'mean',\n",
    "        'probability_positive': ['mean', 'count']\n",
    "    }).reset_index()\n",
    "    calibration.columns = ['prob_bin', 'actual_rate', 'mean_predicted_prob', 'count']\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.scatter(calibration['mean_predicted_prob'], calibration['actual_rate'], \n",
    "                s=calibration['count']*2, alpha=0.6, color='blue')\n",
    "    plt.plot([0, 1], [0, 1], 'r--', label='Perfect Calibration')\n",
    "    plt.xlabel('Mean Predicted Probability', fontsize=12)\n",
    "    plt.ylabel('Actual No-Show Rate', fontsize=12)\n",
    "    plt.title('Calibration Curve\\n(Point size = number of predictions)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n=== Calibration Analysis ===\")\n",
    "    print(calibration.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94082d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_high_confidence_errors(df, confidence_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Find predictions where model was very confident but wrong.\n",
    "    \"\"\"\n",
    "    df['is_error'] = (df['noShow_day1_target'] != df['prediction']).astype(int)\n",
    "    \n",
    "    # High confidence false positives - confidently predicted no-show, but showed\n",
    "    high_conf_fp = df[\n",
    "        (df['is_error'] == 1) & \n",
    "        (df['noShow_day1_target'] == 0) & \n",
    "        (df['probability_positive'] >= confidence_threshold)\n",
    "    ].copy()\n",
    "    \n",
    "    # High confidence false negatives - confidently predicted show, but no-showed\n",
    "    high_conf_fn = df[\n",
    "        (df['is_error'] == 1) & \n",
    "        (df['noShow_day1_target'] == 1) & \n",
    "        (df['probability_positive'] <= (1 - confidence_threshold))\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"\\n=== High Confidence Errors (threshold={confidence_threshold}) ===\")\n",
    "    print(f\"High-confidence False Positives: {len(high_conf_fp)} \"\n",
    "          f\"({len(high_conf_fp)/len(df)*100:.2f}% of all predictions)\")\n",
    "    print(f\"High-confidence False Negatives: {len(high_conf_fn)} \"\n",
    "          f\"({len(high_conf_fn)/len(df)*100:.2f}% of all predictions)\")\n",
    "    \n",
    "    # Plot distribution\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    ax1.hist(high_conf_fp['probability_positive'], bins=30, color='orange', \n",
    "             alpha=0.7, edgecolor='black')\n",
    "    ax1.set_title(f'High-Confidence False Positives (n={len(high_conf_fp)})')\n",
    "    ax1.set_xlabel('Predicted Probability of No-Show')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.axvline(confidence_threshold, color='red', linestyle='--', \n",
    "                label=f'Confidence Threshold: {confidence_threshold}')\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.hist(high_conf_fn['probability_positive'], bins=30, color='red', \n",
    "             alpha=0.7, edgecolor='black')\n",
    "    ax2.set_title(f'High-Confidence False Negatives (n={len(high_conf_fn)})')\n",
    "    ax2.set_xlabel('Predicted Probability of No-Show')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax2.axvline(1 - confidence_threshold, color='red', linestyle='--', \n",
    "                label=f'Confidence Threshold: {1-confidence_threshold}')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return high_conf_fp, high_conf_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f293c766",
   "metadata": {},
   "source": [
    "### Linear Regression Model Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33793306",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pr_curve_data = Dataset.get(\"lr_pr_curve_data\").read_table(format=\"pandas\")\n",
    "lr_conf_matrix_plot_data = Dataset.get(\"lr_conf_matrix_plot_data\").read_table(format=\"pandas\")\n",
    "lr_roc_curve_data = Dataset.get(\"lr_roc_curve_data\").read_table(format=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee48c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR curve plot\n",
    "plot_precision_recall_curve(\n",
    "    lr_pr_curve_data, \n",
    "    figsize=(12, 8)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c43df03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_confusion_matrix(lr_conf_matrix_plot_data, title=\"LR Model Confusion Matrix\")\n",
    "plot.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d57f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_roc_curve(lr_roc_curve_data, title=\"LR Model PROC Curve\")\n",
    "plot.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a05666",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Model Eval Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959ae76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pr_curve_data = Dataset.get(\"mlp_pr_curve_data\").read_table(format=\"pandas\")\n",
    "mlp_conf_matrix_plot_data = Dataset.get(\"mlp_conf_matrix_plot_data\").read_table(format=\"pandas\")\n",
    "mlp_roc_curve_data = Dataset.get(\"mlp_roc_curve_data\").read_table(format=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75ddcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(\n",
    "    mlp_pr_curve_data, \n",
    "    figsize=(12, 8)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d07efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_confusion_matrix(mlp_conf_matrix_plot_data, title=\"MLP Model Confusion Matrix\")\n",
    "plot.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed151d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_roc_curve(mlp_roc_curve_data, title=\"MLP Model PROC Curve\")\n",
    "plot.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae3088",
   "metadata": {},
   "source": [
    "### Random Forest Model Eval Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3324ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pr_curve_data = Dataset.get(\"rf1_pr_curve_data\").read_table(format=\"pandas\")\n",
    "rf_conf_matrix_plot_data = Dataset.get(\"rf1_conf_matrix_plot_data\").read_table(format=\"pandas\")\n",
    "rf_roc_curve_data = Dataset.get(\"rf1_roc_curve_data\").read_table(format=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1741eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(\n",
    "    rf_pr_curve_data, \n",
    "    figsize=(12, 8)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fadc5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_confusion_matrix(rf_conf_matrix_plot_data, title=\"RF Model Confusion Matrix\")\n",
    "plot.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1624e083",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_roc_curve(rf_roc_curve_data, title=\"RF Model PROC Curve\")\n",
    "plot.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b375ef2e",
   "metadata": {},
   "source": [
    "### Gradient-Boosted Tree Model Eval Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd106f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_pr_curve_data = Dataset.get(\"gbt1_pr_curve_data\").read_table(format=\"pandas\")\n",
    "gbt_conf_matrix_plot_data = Dataset.get(\"gbt1_conf_matrix_plot_data\").read_table(format=\"pandas\")\n",
    "gbt_roc_curve_data = Dataset.get(\"gbt1_roc_curve_data\").read_table(format=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce87d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(\n",
    "    gbt_pr_curve_data, \n",
    "    figsize=(12, 8)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5056a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_confusion_matrix(gbt_conf_matrix_plot_data, title=\"GBT Model Confusion Matrix\")\n",
    "plot.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a565bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_roc_curve(gbt_roc_curve_data, title=\"GBT Model PROC Curve\")\n",
    "plot.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8363e81b",
   "metadata": {},
   "source": [
    "### Support Vector Machin Model Eval Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f072023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pr_curve_data = Dataset.get(\"svm_pr_curve_data\").read_table(format=\"pandas\")\n",
    "svm_conf_matrix_plot_data = Dataset.get(\"svm_conf_matrix_plot_data\").read_table(format=\"pandas\")\n",
    "svm_roc_curve_data = Dataset.get(\"svm_roc_curve_data\").read_table(format=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ddcd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(\n",
    "    svm_pr_curve_data, \n",
    "    figsize=(12, 8)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c5f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_confusion_matrix(svm_conf_matrix_plot_data, title=\"SVM Model Confusion Matrix\")\n",
    "plot.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_roc_curve(svm_roc_curve_data, title=\"SVM Model PROC Curve\")\n",
    "plot.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbace19",
   "metadata": {},
   "source": [
    "### Random Forest Model Feature Importance & Error Eval Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddfc68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_feature_importance = Dataset.get(\"rf_feature_importance\").read_table(format=\"pandas\")\n",
    "rf_predictions = Dataset.get(\"rf_predictions\").read_table(format=\"pandas\")\n",
    "rf1_lift_chart_data = Dataset.get(\"rf1_lift_chart_data\").read_table(format=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8154e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cumulative_importance(rf_feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f851f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_calibration_curve(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159d50bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_conf_fp, high_conf_fn = analyze_high_confidence_errors(rf_predictions, confidence_threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66292cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lift_chart_fig = plot_lift_chart(\n",
    "    lift_df=rf1_lift_chart_data,\n",
    "    title=\"RF Model Performance: Lift and Cumulative Gain\",\n",
    "    figsize=(14, 7),\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "lift_chart_fig.show();"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
